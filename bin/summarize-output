#!/usr/bin/env python

import argparse
import logging
from collections import defaultdict

from Bio import SeqIO
import pandas as pd
from runstats import Statistics

def parse_args():
    '''
    return arguments
    >>> args = parse_args()

    '''

    parser = argparse.ArgumentParser(
            description = '''
Summarize results of running pipeline. Good for QC.
Also, you need pandas. And runstats
            ''')
    parser.add_argument('--log', default='/dev/stderr',
                        help='log file (default=stderr)')
    parser.add_argument('--output', default='/dev/stdout')
    parser.add_argument('--directory', '-d', help='pipeline output directory')
    return parser.parse_args()


def main():
    '''
        >>> main() # stuff happens
    '''

    args = parse_args()
    logging.basicConfig(filename=args.log, level=logging.INFO)

    # locate files

    assembled = '%s/assembled.fasta' % args.directory
    labelled = '%s/labelled.fasta' % args.directory
    uc_file = '%s/labelled.uc' % args.directory
    otu_table = '%s/labelled.csv' % args.directory

    print '==> PIPELINE STATISTICS'
    print 'output directory = %s' % args.directory
    print 'pandaseq assembled reads = %s' % assembled
    print 'labelled reads = %s' % labelled
    print 'classified reads = %s' % uc_file
    print 'OTU table = %s' % otu_table

    #
    # PANDASEQ
    #

    lengths = Statistics()
    with open(assembled) as handle:
        records = SeqIO.parse(handle, format='fasta')
        for record in records:
            lengths.push(len(record))

    print
    print '==> PANDASEQ'
    print 'reads assembled by pandaseq = %s' % len(lengths)
    print 'average length = %.2f (SD = %.2f)' % (lengths.mean(),
                                                 lengths.stddev())

    #
    # DEMULTIPLEXING
    #

    reads_per_barcode = defaultdict(lambda: 0)
    with open(labelled) as handle:
        records = SeqIO.parse(handle, format='fasta')
        for record in records:
            bc = record.id.split(':')[-1]
            reads_per_barcode[bc] += 1

    counts = Statistics()
    [ counts.push(i) for i in reads_per_barcode.values() ]

    print
    print '==> DEMULTIPLEXING'
    print 'total barcodes = %s' % (len(counts))
    print 'average reads per barcode = %.2f (SD=%.2f)' % (counts.mean(), counts.stddev())
    print 'max reads per barcode = %s' % counts.maximum()
    print 'min reads per barcode = %s' % counts.minimum()

    #
    # CLASSIFICATION
    #
    stats_by_sample = defaultdict(lambda: { 'hit': 0, 'miss': 0 })

    identity_stats = Statistics()
    length_stats = Statistics()
    with open(uc_file) as handle:
        for line in handle:
            line = line.strip().split("\t")
            if line[0] == 'H':
                kind = 'hit'
            elif line[0] == 'N':
                kind = 'miss'
            else:
                continue

            sample = line[1]

            if kind == 'hit':
                identity_stats.push(float(line[3]))
                length_stats.push(float(line[6]))

            stats_by_sample[sample][kind] += 1

    hits = [ i['hit'] for i in stats_by_sample.values() ]
    misses = [ i['miss'] for i in stats_by_sample.values() ]

    hits_stats = Statistics(hits)
    misses_stats = Statistics(misses)

    print
    print '==> CLASSIFICATION'
    print 'samples with hits = %s' % len(stats_by_sample.keys())
    print 'total hits = %s' % sum(hits)
    print 'total misses = %s' % sum(misses)
    print 'avg hits per sample = %.2f (SD = %.2f)' % (hits_stats.mean(),
                                                      hits_stats.stddev())
    print 'min/max hits per sample = %s/%s' % (min(hits), max(hits))
    print 'avg misses per sample = %.2f (SD = %.2f)' % (misses_stats.mean(),
                                                        misses_stats.stddev())
    print 'min/max misses per sample = %s/%s' % (min(misses), max(misses))
    print 'average identity = %.2f (SD = %.2f)' % (identity_stats.mean(),
                                                   identity_stats.stddev())
    print 'min identity = %s' % identity_stats.minimum()
    print 'max identity = %s' % identity_stats.maximum()

    print 'average length = %.2f (SD = %.2f)' % (length_stats.mean(),
                                                 length_stats.stddev())
    print 'min length = %i' % length_stats.minimum()
    print 'max length = %i' % length_stats.maximum()

    otus = pd.read_csv(otu_table)

    print
    print '==> OTU TABLE'

    print '%s OTUs' % len(otus.columns)
    print '%s samples' % len(otus.index)
    print '%s total reads' % otus.values.sum()


if __name__ == '__main__':
    main()
